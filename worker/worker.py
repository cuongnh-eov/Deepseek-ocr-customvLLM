"""
worker/worker.py
================
Vai tr√≤: ch·∫°y OCR "n·∫∑ng" (GPU) theo message t·ª´ RabbitMQ.

Lu·ªìng:
1) Consume message {"job_id": "..."} t·ª´ queue.
2) Load job t·ª´ DB -> l·∫•y input_path.
3) Update DB: status=RUNNING + (gpu_name, gpu_total_mb).
4) Reset peak VRAM stats -> ch·∫°y pipeline.
5) Save outputs (md/json) -> update DB: SUCCESS + metrics.
6) N·∫øu l·ªói -> update DB: FAILED + error + (VRAM peak n·∫øu ƒë·ªçc ƒë∆∞·ª£c).

Ghi metrics:
- processing_time: t·ªïng th·ªùi gian job
- t_pdf2img, t_preprocess, t_infer, t_postprocess: theo stage (n·∫øu DB c√≥ c·ªôt)
- vram_peak_mb / vram_reserved_peak_mb: peak memory stats (torch.cuda)
"""
from datetime import datetime, timezone
import os
import json
import time
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple

import pika
import torch
import json as pyjson
from sqlalchemy.orm import Session

from app.db import SessionLocal
from app.models import OCRJob, JobStatus

# ===== Reuse pipeline b·∫°n ƒëang c√≥ =====
from configs.config import PROMPT  # prompt OCR
from app.utils import pdf_to_images_high_quality
from app.processor import preprocess_batch, generate_ocr
from app.model_init import llm, sampling_params
from app.postprocess_md import process_ocr_output
from app.postprocess_json import process_ocr_to_blocks
from app.postprocess_md import upload_to_minio

from configs.config import RABBIT_URL, QUEUE_NAME, OUTPUT_PATH

from app.postprocess_md import upload_to_minio


def get_gpu_info() -> Tuple[Optional[str], Optional[int]]:
    """
    L·∫•y th√¥ng tin GPU ƒëang d√πng.
    Return: (gpu_name, total_mb) ho·∫∑c (None, None) n·∫øu kh√¥ng c√≥ CUDA.
    """
    if not torch.cuda.is_available():
        return None, None
    idx = torch.cuda.current_device()
    name = torch.cuda.get_device_name(idx)
    total_mb = int(torch.cuda.get_device_properties(idx).total_memory / (1024 * 1024))
    return name, total_mb


def reset_gpu_peak():
    """
    Reset peak memory stats ƒë·ªÉ ƒëo "peak VRAM" ch√≠nh x√°c cho t·ª´ng job.
    """
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()


def read_gpu_peak_mb() -> Tuple[Optional[int], Optional[int]]:
    """
    Read peak memory used during job.
    - allocated: memory do tensors allocate
    - reserved : memory cached by CUDA allocator
    """
    if not torch.cuda.is_available():
        return None, None
    peak_alloc = int(torch.cuda.max_memory_allocated() / (1024 * 1024))
    peak_resv = int(torch.cuda.max_memory_reserved() / (1024 * 1024))
    return peak_alloc, peak_resv


def extract_content(text: str, job_id: str) -> str:
    """
    L√†m s·∫°ch output raw c·ªßa model theo logic b·∫°n ƒëang d√πng:
    - b·ªè end-of-sentence token
    - thay <|ref|>image... b·∫±ng markdown image placeholder
    - xo√° c√°c ref/det kh√°c
    - chu·∫©n ho√° k√Ω hi·ªáu latex
    """
    if "<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>" in text:
        text = text.replace("<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>", "")

    pattern = r'(<\|ref\|>(.*?)<\|/ref\|><\|det\|>(.*?)<\|/det\|>)'
    matches = re.findall(pattern, text, re.DOTALL)

    matches_image, matches_other = [], []
    for a_match in matches:
        if "<|ref|>image<|/ref|>" in a_match[0]:
            matches_image.append(a_match[0])
        else:
            matches_other.append(a_match[0])

    for img_idx, match in enumerate(matches_image):
        text = text.replace(match, f"![](./{job_id}/images/{img_idx}.jpg)\n")

    for match in matches_other:
        text = text.replace(match, "")

    text = text.replace("\\coloneqq", ":=").replace("\\eqqcolon", "=:")
    text = text.replace("\n\n\n\n", "\n\n").replace("\n\n\n", "\n\n")
    return text


def update_job(db: Session, job: OCRJob, **kwargs):
    """
    Helper c·∫≠p nh·∫≠t job + updated_at r·ªìi commit.
    """
    for k, v in kwargs.items():
        # N·∫øu DB model kh√¥ng c√≥ field k th√¨ b·ªè qua (tr√°nh crash khi b·∫°n ch∆∞a migrate)
        if hasattr(job, k):
            setattr(job, k, v)
    job.updated_at = datetime.now(timezone.utc)
    db.add(job)
    db.commit()


def process_one_job(job_id: str):
    """
    X·ª≠ l√Ω 1 job theo job_id: PDF -> Images -> AI -> Crop -> MD -> Cleanup
    """
    db = SessionLocal()
    job = db.get(OCRJob, job_id)

    if not job:
        db.close()
        return

    gpu_name, gpu_total_mb = get_gpu_info()

    try:
        # 1. Update tr·∫°ng th√°i ƒëang ch·∫°y
        # L·∫•y file size ƒë·ªÉ ghi metrics
        file_size = 0
        if os.path.exists(job.input_path):
            file_size = round(os.path.getsize(job.input_path) / (1024 * 1024), 2)

        update_job(
            db, job,
            status=JobStatus.RUNNING,
            error=None,
            gpu_name=gpu_name,
            gpu_total_mb=gpu_total_mb,
            file_size_mb=file_size
        )

        reset_gpu_peak()
        t0 = time.time()

        # 2. T·∫°o th∆∞ m·ª•c output
        output_dir = os.path.join(OUTPUT_PATH, job_id)
        os.makedirs(output_dir, exist_ok=True)

        # 3. PDF -> Images
        t_pdf2img0 = time.time()
        images = pdf_to_images_high_quality(job.input_path)
        t_pdf2img = time.time() - t_pdf2img0
        total_pages = len(images)

        # 4. AI Inference (Preprocess + Model)
        t_pre0 = time.time()
        batch_inputs = preprocess_batch(images, PROMPT)
        t_preprocess = time.time() - t_pre0

        t_inf0 = time.time()
        outputs = generate_ocr(llm, batch_inputs, sampling_params)
        t_infer = time.time() - t_inf0

        # 5. H·∫≠u x·ª≠ l√Ω (Post-process)
        t_post0 = time.time()
        
        # --- QUAN TR·ªåNG ---
        # H√†m n√†y s·∫Ω t·ª± CROP ·∫£nh nh·ªè v√† T·ª∞ L∆ØU file Markdown v√†o output_dir
        markdown_text, _, _ = process_ocr_output(outputs, images, out_path=output_dir)
        
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n file MD (t√™n file g·ªëc + .md)
      # worker.py
        markdown_path = os.path.join(output_dir, f"{job_id}.md")
        with open(markdown_path, "w", encoding="utf-8") as f:
            f.write(markdown_text)

        
        # 5.2) T·∫°o JSON blocks c·∫•u tr√∫c
        content_pages = []
        for page_num, output in enumerate(outputs):
            # S·ª≠ d·ª•ng n·ªôi dung ƒë√£ s·∫°ch t·ª´ h√†m extract_content
            cleaned = extract_content(output.outputs[0].text, job_id)
            blocks = process_ocr_to_blocks(cleaned)
            content_pages.append({"page_number": page_num + 1, "blocks": blocks})

        response_data = {
            "document": {
                "metadata": {
                    "source_filename": job.filename,
                    "total_pages": total_pages,
                    "processed_at": datetime.now(timezone.utc).isoformat(),
                },
                "content": content_pages,
            }
        }

        json_path = os.path.join(output_dir, f"{job_id}.json")
        with open(json_path, "w", encoding="utf-8") as f:
            pyjson.dump(response_data, f, ensure_ascii=False, indent=2)

        # B∆Ø·ªöC M·ªöI: ƒê·∫®Y L√äN MINIO T·ª™ WORKER
        # =======================================================
        print(f"üöÄ ƒêang ƒë·∫©y k·∫øt qu·∫£ Job {job_id} l√™n MinIO...")
        try:
            # upload_dir ch√≠nh l√† output_dir ch·ª©a c·∫£ md, json v√† images
            upload_to_minio(output_dir, job_id)
            print(f"‚úÖ ƒê√£ t·∫£i l√™n MinIO th√†nh c√¥ng.")
        except Exception as minio_err:
            print(f"‚ö†Ô∏è L·ªói Upload MinIO nh∆∞ng v·∫´n ti·∫øp t·ª•c: {minio_err}")

        # =======================================================
        t_postprocess = time.time() - t_post0
        processing_time = time.time() - t0
        vram_peak_mb, vram_reserved_peak_mb = read_gpu_peak_mb()

        # 6. C·∫≠p nh·∫≠t DB th√†nh c√¥ng
        update_job(
            db, job,
            status=JobStatus.SUCCESS,
            num_pages=total_pages,
            processing_time=round(processing_time, 3),
            vram_peak_mb=vram_peak_mb,
            vram_reserved_peak_mb=vram_reserved_peak_mb,
            t_pdf2img=round(t_pdf2img, 3),
            t_preprocess=round(t_preprocess, 3),
            t_infer=round(t_infer, 3),
            t_postprocess=round(t_postprocess, 3),
            output_dir=output_dir,
            markdown_path=markdown_path,
            json_path=json_path,
        )

        # --- D·ªåN D·∫∏P FILE T·∫†M ---
        # Sau khi SUCCESS, x√≥a file PDF g·ªëc trong uploads ƒë·ªÉ ti·∫øt ki·ªám ·ªï c·ª©ng
        if os.path.exists(job.input_path):
            os.remove(job.input_path)
            print(f"‚úÖ ƒê√£ d·ªçn d·∫πp file PDF g·ªëc: {job.input_path}")

    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω Job {job_id}: {str(e)}")
        vram_peak_mb, vram_reserved_peak_mb = read_gpu_peak_mb()
        update_job(
            db, job,
            status=JobStatus.FAILED,
            error=str(e),
            vram_peak_mb=vram_peak_mb,
            vram_reserved_peak_mb=vram_reserved_peak_mb,
        )
    finally:
        db.close()

def main():
    """
    Worker loop:
    - prefetch_count=1 => m·ªói worker x·ª≠ 1 job t·∫°i 1 th·ªùi ƒëi·ªÉm.
      R·∫§T quan tr·ªçng n·∫øu b·∫°n ch·∫°y 1 GPU nh·ªè (RTX 3060) ƒë·ªÉ tr√°nh nhi·ªÅu job ƒÉn VRAM c√πng l√∫c.
    - ack sau khi x·ª≠ l√Ω xong -> n·∫øu worker ch·∫øt gi·ªØa ch·ª´ng th√¨ RabbitMQ c√≥ th·ªÉ requeue message.
    """
    params = pika.URLParameters(RABBIT_URL)
    conn = pika.BlockingConnection(params)
    ch = conn.channel()

    # durable queue: queue kh√¥ng m·∫•t khi restart (tu·ª≥ c·∫•u h√¨nh persistence RabbitMQ)
    ch.queue_declare(queue=QUEUE_NAME, durable=True)

    # M·ªói worker ch·ªâ nh·∫≠n 1 job t·∫°i 1 th·ªùi ƒëi·ªÉm
    ch.basic_qos(prefetch_count=1)

    def callback(ch, method, properties, body):
        msg = json.loads(body.decode("utf-8"))
        job_id = msg.get("job_id")
        if not job_id:
            # message l·ªói format -> ack v√† b·ªè
            ch.basic_ack(delivery_tag=method.delivery_tag)
            return

        process_one_job(job_id)

        # ACK sau khi l√†m xong
        ch.basic_ack(delivery_tag=method.delivery_tag)

    ch.basic_consume(queue=QUEUE_NAME, on_message_callback=callback)
    print(f"‚úÖ Worker listening: queue={QUEUE_NAME}")
    ch.start_consuming()


if __name__ == "__main__":
    main()
