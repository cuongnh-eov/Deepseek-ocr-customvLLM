"""
worker/worker.py
================
Vai tr√≤: ch·∫°y OCR "n·∫∑ng" (GPU) theo message t·ª´ RabbitMQ.
Lu·ªìng:
1) Consume message {"job_id": "..."} t·ª´ queue.
2) Load job t·ª´ DB -> l·∫•y input_path.
3) Update DB: status=RUNNING + (gpu_name, gpu_total_mb).
4) Reset peak VRAM stats -> ch·∫°y pipeline.
5) Save outputs (md/json) -> update DB: SUCCESS + metrics.
6) N·∫øu l·ªói -> update DB: FAILED + error + (VRAM peak n·∫øu ƒë·ªçc ƒë∆∞·ª£c).
Ghi metrics:
- processing_time: t·ªïng th·ªùi gian job
- t_pdf2img, t_preprocess, t_infer, t_postprocess: theo stage (n·∫øu DB c√≥ c·ªôt)
- vram_peak_mb / vram_reserved_peak_mb: peak memory stats (torch.cuda)
"""
from datetime import datetime, timezone
import os
import time
import re
import torch
import json as pyjson
from pathlib import Path
from typing import Optional, Tuple
from sqlalchemy.orm import Session

# --- Import t·ª´ Core ---
from app.core.db import SessionLocal
from app.core.models import OCRJob, JobStatus
from app.core.config import (
    PROMPT, OUTPUT_PATH, 
    MINIO_ENDPOINT, MINIO_BUCKET_NAME
)

# --- Import t·ª´ Services & Utils ---
# L∆∞u √Ω: Ki·ªÉm tra ch√≠nh x√°c processor.py n·∫±m ·ªü ƒë√¢u (Tree c·ªßa b·∫°n ghi services)
from app.services.processor import preprocess_batch, generate_ocr, run_tesseract_fallback
from app.services.publisher import send_finished_notification

from app.utils.utils import pdf_to_images_high_quality
from app.utils.postprocess_md import process_ocr_output, upload_to_minio
from app.utils.postprocess_json import process_ocr_to_blocks

# --- Import t·ª´ Worker (C√πng folder) ---
from worker.model_init import llm, sampling_params
def get_gpu_info() -> Tuple[Optional[str], Optional[int]]:
    """
    L·∫•y th√¥ng tin GPU ƒëang d√πng.
    Return: (gpu_name, total_mb) ho·∫∑c (None, None) n·∫øu kh√¥ng c√≥ CUDA.
    """
    if not torch.cuda.is_available():
        return None, None
    idx = torch.cuda.current_device()
    name = torch.cuda.get_device_name(idx)
    total_mb = int(torch.cuda.get_device_properties(idx).total_memory / (1024 * 1024))
    return name, total_mb
def reset_gpu_peak():
    """
    Reset peak memory stats ƒë·ªÉ ƒëo "peak VRAM" ch√≠nh x√°c cho t·ª´ng job.
    """
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
def read_gpu_peak_mb() -> Tuple[Optional[int], Optional[int]]:
    """
    Read peak memory used during job.
    - allocated: memory do tensors allocate
    - reserved : memory cached by CUDA allocator
    """
    if not torch.cuda.is_available():
        return None, None
    peak_alloc = int(torch.cuda.max_memory_allocated() / (1024 * 1024))
    peak_resv = int(torch.cuda.max_memory_reserved() / (1024 * 1024))
    return peak_alloc, peak_resv


def extract_content(text: str, job_id: str) -> str:
    """
    L√†m s·∫°ch output raw c·ªßa model theo logic b·∫°n ƒëang d√πng:
    - b·ªè end-of-sentence token
    - thay <|ref|>image... b·∫±ng markdown image placeholder
    - xo√° c√°c ref/det kh√°c
    - chu·∫©n ho√° k√Ω hi·ªáu latex
    """
    if "<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>" in text:
        text = text.replace("<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>", "")
    pattern = r'(<\|ref\|>(.*?)<\|/ref\|><\|det\|>(.*?)<\|/det\|>)'
    matches = re.findall(pattern, text, re.DOTALL)
    matches_image, matches_other = [], []
    for a_match in matches:
        if "<|ref|>image<|/ref|>" in a_match[0]:
            matches_image.append(a_match[0])
        else:
            matches_other.append(a_match[0])
    for img_idx, match in enumerate(matches_image):
        text = text.replace(match, f"![](./{job_id}/images/{img_idx}.jpg)\n")
    for match in matches_other:
        text = text.replace(match, "")
    text = text.replace("\\coloneqq", ":=").replace("\\eqqcolon", "=:")
    text = text.replace("\n\n\n\n", "\n\n").replace("\n\n\n", "\n\n")
    return text
def update_job(db: Session, job: OCRJob, **kwargs):
    """
    Helper c·∫≠p nh·∫≠t job + updated_at r·ªìi commit.
    """
    for k, v in kwargs.items():
        # N·∫øu DB model kh√¥ng c√≥ field k th√¨ b·ªè qua (tr√°nh crash khi b·∫°n ch∆∞a migrate)
        if hasattr(job, k):
            setattr(job, k, v)
    job.updated_at = datetime.now(timezone.utc)
    db.add(job)
    db.commit()
def process_one_job(job_id: str):
    db = SessionLocal()
    job = db.get(OCRJob, job_id)

    if not job:
        db.close()
        return

    gpu_name, gpu_total_mb = get_gpu_info()

    try:
        # 1. Kh·ªüi t·∫°o v√† Update tr·∫°ng th√°i RUNNING
        file_size = 0
        if os.path.exists(job.input_path):
            file_size = round(os.path.getsize(job.input_path) / (1024 * 1024), 2)

        update_job(db, job, status=JobStatus.RUNNING, gpu_name=gpu_name, gpu_total_mb=gpu_total_mb, file_size_mb=file_size)
        reset_gpu_peak()
        t0 = time.time()

        # 2. Th∆∞ m·ª•c output
        output_dir = os.path.join(OUTPUT_PATH, job_id)
        os.makedirs(output_dir, exist_ok=True)

        # 3. PDF -> Images
        t_pdf2img0 = time.time()
        images = pdf_to_images_high_quality(job.input_path)
        t_pdf2img = time.time() - t_pdf2img0
        total_pages = len(images)

        # 4. AI Inference
        t_pre0 = time.time()
        batch_inputs = preprocess_batch(images, PROMPT)
        t_preprocess = time.time() - t_pre0

        t_inf0 = time.time()
        outputs = generate_ocr(llm, batch_inputs, sampling_params)
        t_infer = time.time() - t_inf0

        # 5. H·∫¨U X·ª¨ L√ù (S·ª≠ d·ª•ng l·∫°i logic JSON ·ªïn ƒë·ªãnh c·ªßa b·∫°n)
        t_post0 = time.time()
        
        # 5.1) T·∫°o file Markdown
        # L∆∞u √Ω: H√†m n√†y c·ªßa b·∫°n th∆∞·ªùng t·ª± l∆∞u v√†o output_dir
        markdown_text, _, _ = process_ocr_output(outputs, images, out_path=output_dir)
        markdown_path = os.path.join(output_dir, f"{job_id}.md")
        with open(markdown_path, "w", encoding="utf-8") as f:
            f.write(markdown_text)

        # 5.2) T·∫°o file JSON (S·ª≠ d·ª•ng logic t·ª´ file c≈© b·∫°n g·ª≠i)
        content_pages = []
        for page_num, output in enumerate(outputs):
            # L·∫•y text raw t·ª´ output c·ªßa vLLM
            raw_text = output.outputs[0].text if hasattr(output, 'outputs') else str(output)
            # L√†m s·∫°ch b·∫±ng h√†m extract_content c√≥ s·∫µn trong file n√†y
            cleaned = extract_content(raw_text, job_id)
            # Chuy·ªÉn ƒë·ªïi sang blocks
            blocks = process_ocr_to_blocks(cleaned)
            content_pages.append({"page_number": page_num + 1, "blocks": blocks})

        response_data = {
            "document": {
                "metadata": {
                    "source_filename": job.filename,
                    "total_pages": total_pages,
                    "processed_at": datetime.now(timezone.utc).isoformat(),
                },
                "content": content_pages,
            }
        }

        json_path = os.path.join(output_dir, f"{job_id}.json")
        with open(json_path, "w", encoding="utf-8") as f:
            pyjson.dump(response_data, f, ensure_ascii=False, indent=2)
            f.flush()
            os.fsync(f.fileno()) # ƒê·∫£m b·∫£o file ƒë∆∞·ª£c ghi xu·ªëng ƒëƒ©a tr∆∞·ªõc khi upload

        # 6. T·∫£i l√™n MinIO (S·ª≠ d·ª•ng h√†m qu√©t to√†n b·ªô th∆∞ m·ª•c c·ªßa b·∫°n)
        print(f"üöÄ ƒêang ƒë·∫©y k·∫øt qu·∫£ Job {job_id} l√™n MinIO...")
        upload_to_minio(output_dir, job_id)

        t_postprocess = time.time() - t_post0
        vram_peak_mb, vram_resv = read_gpu_peak_mb()

        # 7. C·∫≠p nh·∫≠t th√†nh c√¥ng
        update_job(
            db, job,
            status=JobStatus.SUCCESS,
            num_pages=total_pages,
            processing_time=round(time.time() - t0, 3),
            vram_peak_mb=vram_peak_mb,
            t_pdf2img=round(t_pdf2img, 3),
            t_preprocess=round(t_preprocess, 3),
            t_infer=round(t_infer, 3),
            t_postprocess=round(t_postprocess, 3),
            result_path=f"{MINIO_ENDPOINT}/{MINIO_BUCKET_NAME}/{job_id}/{job_id}.md",
            minio_json_url=f"{MINIO_ENDPOINT}/{MINIO_BUCKET_NAME}/{job_id}/{job_id}.json"
        )
        
        # 8. Th√¥ng b√°o (N·∫øu b·∫°n ƒë√£ s·ª≠a l·ªói vhost RabbitMQ)
        try:
            from app.publisher import send_finished_notification
            send_finished_notification(job_id)
        except:
            pass

    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω Job {job_id}: {str(e)}")
        vram_peak_mb, _ = read_gpu_peak_mb()
        update_job(db, job, status=JobStatus.FAILED, error=str(e), vram_peak_mb=vram_peak_mb)
    finally:
        if os.path.exists(job.input_path):
            os.remove(job.input_path)
        db.close()